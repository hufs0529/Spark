{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F15IlqDIaqNp"
      },
      "outputs": [],
      "source": [
        "# 설치\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8tT2iw6GVyTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RDD"
      ],
      "metadata": {
        "id": "S7xGkjwFgg-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf"
      ],
      "metadata": {
        "id": "N6WoetuGa1rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SparkConf: 사용자가 재정의해서 쓸 수 있는 설정 옵션들에 대한 키와 값을 갖고 있는 객체\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"practice\")\n",
        "# SparkContext: Spark 클러스터와 연결시켜주는 객체. RDD 생성을 위해 필요\n",
        "sc = SparkContext('local', 'practice')    # SparkContext(conf)\n"
      ],
      "metadata": {
        "id": "u-78AfAOdYa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=[(\"Z\", 1),(\"A\", 20),(\"B\", 30),(\"C\", 40),(\"B\", 30),(\"B\", 60)]"
      ],
      "metadata": {
        "id": "TMPsopMqa80u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parallelize: RDD를 생성하는 메서드\n",
        "inputRDD = sc.parallelize(data)\n",
        "print(inputRDD)\n",
        "inputRDD.collect()\n",
        "# collect: RDD에 포함되어 있는 모든 요소를 반환하는 리스트\n",
        "py_rdd = inputRDD.collect()\n",
        "\n",
        "for i in py_rdd:\n",
        "   print(i[0] + \",\" + str(i[1]))"
      ],
      "metadata": {
        "id": "CVTmvmcXa_4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listRdd = sc.parallelize([1,2,3,4,5,3,2])\n",
        "listRdd.collect()\n",
        "\n",
        "# listRdd 개수 새기\n",
        "print(\"Count : \"+str(listRdd.count()))\n",
        "\n",
        "# countByValue() 함수는 RDD에 포함된 요소의 개수 합계를 반환(딕셔너리 형태)\n",
        "print(\"countByValue :  \"+str(listRdd.countByValue()))"
      ],
      "metadata": {
        "id": "xV8sS-9AbAEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputRDD.collect()\n",
        "listRdd.collect()\n",
        "\n",
        "# first() 함수는 RDD에 포함된 요소 중 첫 번째 반환\n",
        "print(\"first :  \"+str(listRdd.first()))\n",
        "print(\"first :  \"+str(inputRDD.first()))"
      ],
      "metadata": {
        "id": "4zRUx5DkbAc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputRDD.collect()\n",
        "listRdd.collect()\n",
        "\n",
        "print(\"top : \"+str(listRdd.top(2)))\n",
        "\n",
        "print(\"top : \"+str(inputRDD.top(2)))\n",
        "\n",
        "print(\"min :  \"+str(listRdd.min()))\n",
        "\n",
        "print(\"min :  \"+str(inputRDD.min()))\n",
        "\n",
        "print(\"max :  \"+str(listRdd.max()))\n",
        "\n",
        "print(\"max :  \"+str(inputRDD.max()))"
      ],
      "metadata": {
        "id": "vXy_GI9qbAmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 리스트 생성\n",
        "rdd = sc.parallelize(['a,b,c','d,e,f','g,h,i'])\n",
        "\n",
        "# map() 함수는 자료구조 형태의 모든 요소에 적용 후 출력\n",
        "map_rdd = rdd.map(lambda x: x.split(','))\n",
        "print(map_rdd.collect())\n",
        "\n",
        "# flatmap() 함수는 자료구조 형태의 모든 요소에 적용하고, 평면화(flat)하여 출력\n",
        "flatmap_rdd = rdd.flatMap(lambda x: x.split(','))\n",
        "print(flatmap_rdd.collect())\n",
        "\n"
      ],
      "metadata": {
        "id": "DPxMJNWEfSIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 튜플 생성\n",
        "data=[(\"Z\", 1),(\"A\", 20),(\"B\", 30),(\"C\", 40),(\"B\", 30),(\"B\", 60)]\n",
        "inputRDD = sc.parallelize(data)\n",
        "\n",
        "# reduceByKey() 함수는 Key, Value 형태로 데이터가 구성되어 있을 때 Key를 기준으로 합계를 연산\n",
        "rdd2=inputRDD.reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "rdd2.collect()\n",
        "\n",
        "for element in rdd2.collect():\n",
        "    print(element)"
      ],
      "metadata": {
        "id": "GtZFQ0jnfSQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataFrame & PySQL"
      ],
      "metadata": {
        "id": "Jh9w6I3jgj1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "frPZa_75hSwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### SparkSession -> DataFrame 생성 위해 필요\n",
        "\n",
        "# RDD를 생성하기 위해 SparkContext가 필요했던 것처럼 데이터프레임을 생성하기 위해서는 SparkSession을 이용해야 한다. \n",
        "# SparkSession은 인스턴스 생성을 위한 build() 메서드를 제공하고, 이 메서드를 이용하면 기존 인스턴스를 재사용하거나 새로운 인스턴스를 생성할 수 있다"
      ],
      "metadata": {
        "id": "38NI8biM0S4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# config 환경 설정 파일\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark SQL basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "C0xQ6vd5hFdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "# columns 변수에 리스트 형태로 데이터를 준비\n",
        "columns = [\"language\",\"users_count\"]\n",
        "# data 변수에 리스트 내 튜플 형태의 데이터 준비\n",
        "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# 컬럼 지정 x (rdd -> df)\n",
        "dfFromRDD = rdd.toDF()\n",
        "dfFromRDD.printSchema()\n",
        "\n",
        "# 스키마 구조 확인을 위해 printSchema()사용\n",
        "\n",
        "# 컬럼 지정\n",
        "dfFromRDD1 = rdd.toDF(columns)\n",
        "dfFromRDD1.printSchema()\n",
        "\n",
        "# df 바로 생성하기\n",
        "dfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\n",
        "dfFromRDD2.printSchema()"
      ],
      "metadata": {
        "id": "mfVMTEoYfSYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# StructType를 사용해 스키마를 먼저 생성하고 df 생성하기\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
        "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
        "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
        "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
        "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
        "]\n",
        "\n",
        "schema = StructType([ \n",
        "    StructField(\"firstname\",StringType(),True), \n",
        "    StructField(\"middlename\",StringType(),True), \n",
        "    StructField(\"lastname\",StringType(),True), \n",
        "    StructField(\"id\", StringType(), True), \n",
        "    StructField(\"gender\", StringType(), True), \n",
        "    StructField(\"salary\", IntegerType(), True)\n",
        "  ])\n",
        " \n",
        "df = spark.createDataFrame(data=data2,schema=schema)\n",
        "df.printSchema()\n",
        "# truncate=False로 생략되는 부분 없이 모두 보여주기\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "GDK_Q-NUfSfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "f = files.upload()"
      ],
      "metadata": {
        "id": "1f26981NfSlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = spark.read.csv('animal_data_img.csv')\n",
        "df2.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "Mn112nOQfSrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 빈 RDD생성\n",
        "\n",
        "emptyRDD = spark.sparkContext.emptyRDD()\n",
        "print(emptyRDD)\n",
        "rdd2= spark.sparkContext.parallelize([])\n",
        "print(rdd2)"
      ],
      "metadata": {
        "id": "mRwl3kNKfSw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스키마 구조와 함께 빈 Dataframe 생성\n",
        "# StructType() 함수를 이용하여 스키마 구조를 정의하고, 전 장에서 생성했던 emptyRDD를 createDataFrame() 함수내의 데이터로 입력하여 생성\n",
        "\n",
        "from pyspark.sql.types import StructType,StructField, StringType\n",
        "schema = StructType([\n",
        "  StructField('firstname', StringType(), True),\n",
        "  StructField('middlename', StringType(), True),\n",
        "  StructField('lastname', StringType(), True)\n",
        "])\n",
        "df = spark.createDataFrame(emptyRDD,schema)\n",
        "df.printSchema()\n",
        "\n",
        "df1 = emptyRDD.toDF(schema)\n",
        "df1.printSchema()\n",
        "\n",
        "df2 = spark.createDataFrame([], schema)\n",
        "df2.printSchema()\n",
        "\n",
        "df3 = spark.createDataFrame([], StructType([]))\n",
        "df3.printSchema()"
      ],
      "metadata": {
        "id": "dA08qaoSrc5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RDD to Dataframe\n",
        "# parallelize() 를 통해 RDD 형태로 데이터를 생성했다면, RDD를 저장해둔 변수.toDF(“컬럼명 변수“)를 통해 Dataframe 생성\n",
        "\n",
        "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
        "rdd = spark.sparkContext.parallelize(dept)\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "df2 = rdd.toDF(deptColumns)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "id": "lQ221rcMrsnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RDD to Dataframe\n",
        "# createDataFrame() 함수의 첫 번째 인자를 RDD 변수, 두 번째 인자를 컬럼명들을 저장한 변수로 Dataframe 생성\n",
        "\n",
        "deptDF = spark.createDataFrame(rdd, deptColumns)\n",
        "deptDF.printSchema()\n",
        "deptDF.show(truncate=False)"
      ],
      "metadata": {
        "id": "0vADEZNpr7xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RDD to Dataframe\n",
        "# createDataFrame() 함수의 두 번째 인자를 StructType으로 구성한 구조를 정의해서 생성\n",
        "\n",
        "from pyspark.sql.types import StructType,StructField, StringType\n",
        "deptSchema = StructType([       \n",
        "    StructField('dept_name', StringType(), True),\n",
        "    StructField('dept_id', StringType(), True)\n",
        "])\n",
        "deptDF1 = spark.createDataFrame(rdd, schema = deptSchema)\n",
        "deptDF1.printSchema()\n",
        "deptDF1.show(truncate=False)"
      ],
      "metadata": {
        "id": "SRIKQmGzrzyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe to Pandas\n",
        "\n",
        "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
        "rdd = spark.sparkContext.parallelize(dept)\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "df2 = rdd.toDF(deptColumns)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)\n",
        "\n",
        "import pandas\n",
        "pandasDF = df2.toPandas()\n",
        "print(pandasDF)"
      ],
      "metadata": {
        "id": "jpvXNG0Gsb9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pandas -> df\n",
        "\n",
        "PySparkDF = spark.createDataFrame(pandasDF)\n",
        "\n",
        "PySparkDF.show(truncate=False)"
      ],
      "metadata": {
        "id": "zBeE23TatSEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show 확인해보기\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "columns = [\"Seqno\",\"Quote\"]\n",
        "data = [(\"1\", \"Be the change that you wish to see in the world\"),\n",
        "    (\"2\", \"Everyone thinks of changing the world, but no one thinks of changing himself.\"),\n",
        "    (\"3\", \"The purpose of our lives is to be happy.\"),\n",
        "    (\"4\", \"Be cool.\")]\n",
        "df = spark.createDataFrame(data,columns)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "I4tJZm6Ou6Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(truncate=False)\n",
        "# df.show(2,truncate=False) \n",
        "# df.show(2,truncate=25) \n",
        "# df.show(n=3,truncate=25,vertical=True)"
      ],
      "metadata": {
        "id": "fiV3-MSSvWPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column\n",
        "# select\n",
        "data=[(\"James\",23),(\"Ann\",40)]\n",
        "df=spark.createDataFrame(data).toDF(\"name.fname\",\"gender\")\n",
        "df.printSchema()\n",
        "df.select(df.gender).show()\n",
        "df.select(df[\"gender\"]).show()\n",
        "df.select(df[\"`name.fname`\"]).show()"
      ],
      "metadata": {
        "id": "-0372fiiv2-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column\n",
        "# col\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "df.select(col(\"gender\")).show()\n",
        "df.select(col(\"`name.fname`\")).show()"
      ],
      "metadata": {
        "id": "VKreiIPXv_9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column\n",
        "# Row를 통해 행 생성\n",
        "\n",
        "from pyspark.sql import Row\n",
        "data=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n",
        "      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\n",
        "df=spark.createDataFrame(data)\n",
        "df.printSchema()\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "pvygeEIVwbNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column\n",
        "#생성된 “prop” 컬럼 내부의 점(“.”)을 추가하고,“hair”을 입력하여 중첩 구조로 이루어진 데이터에 접근\n",
        "\n",
        "df.select(df.prop.hair).show()\n",
        "df.select(df[\"prop.hair\"]).show()\n",
        "df.select(col(\"prop.hair\")).show()\n",
        "df.select(col(\"prop.*\")).show()"
      ],
      "metadata": {
        "id": "4TzexoCGwma7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column\n",
        "# 컬럼에 연산자(+, -, *, /, %, <, >, ==)를 이용하여 컬럼끼리의 연산 및 비교를 수행할 수 있음\n",
        "\n",
        "data=[(100,2,1),(200,3,4),(300,4,4)]\n",
        "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n",
        "\n",
        "df.select(df.col1 + df.col2).show()\n",
        "df.select(df.col1 - df.col2).show() \n",
        "df.select(df.col1 * df.col2).show()\n",
        "df.select(df.col1 / df.col2).show()\n",
        "df.select(df.col1 % df.col2).show()\n",
        "df.select(df.col2 > df.col3).show()\n",
        "df.select(df.col2 < df.col3).show()\n",
        "df.select(df.col2 == df.col3).show()\n"
      ],
      "metadata": {
        "id": "OJB57xhQw-GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 새 df생성\n",
        "data=[(\"James\",\"Bond\",\"100\",None),\n",
        "      (\"Ann\",\"Varsa\",\"200\",'F'),\n",
        "      (\"Tom Cruise\",\"XXX\",\"400\",''),\n",
        "      (\"Tom Brand\",None,\"400\",'M')] \n",
        "columns=[\"fname\",\"lname\",\"id\",\"gender\"]\n",
        "df=spark.createDataFrame(data,columns)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "z5GKchFQxRRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column\n",
        "# alias() 함수를 사용하여 별칭을 이용하여 컬럼명을 변경하여 원하는 컬럼만 출력\n",
        "\n",
        "df.select(df.fname.alias(\"first_name\"), df.lname.alias(\"last_name\")).show()"
      ],
      "metadata": {
        "id": "jE3b5tQUxZc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column\n",
        "# expr() 함수를 이용하여 “fname”과 “lname” 컬럼의 데이터를 합쳐셔 표현\n",
        "\n",
        "from pyspark.sql.functions import expr\n",
        "df.select(expr(\" fname ||','|| lname\").alias(\"fullName\")).show()\n"
      ],
      "metadata": {
        "id": "G2SlvDAVxpCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column\n",
        "# asc()와 desc() 함수를 이용하여 컬럼의 데이터를 정렬할 수 있음\n",
        "\n",
        "df.sort(df.fname.asc()).show()\n",
        "df.sort(df.fname.desc()).show()\n"
      ],
      "metadata": {
        "id": "ctQpMkKFyVCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Column\n",
        "# cast() 함수를 이용하여 데이터의 형변환이 가능\n",
        "\n",
        "df.printSchema()\n",
        "df.select(df.fname,df.id.cast(\"int\")).printSchema()\n"
      ],
      "metadata": {
        "id": "IENg7PCxyepi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column\n",
        "# between() 함수를 이용하여 지정된 값의 범위만큼 데이터 출력 가능\n",
        "# contains() 함수를 통해 입력한 문자열이 포함된 행을 출력 가능\n",
        "\n",
        "df.filter(df.id.between(100,300)).show()\n",
        "df.filter(df.fname.contains(\"Cruise\")).show()\n",
        "df.filter(df.fname == \"Cruise\").show()\n",
        "df.filter(df.fname == \"Tom Cruise\").show()\n"
      ],
      "metadata": {
        "id": "ycxJ-WoIyriW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column\n",
        "# startswith() 함수를 통해 특정 문자가 시작되는 행을 검색할 수도 있고, 반대로 endswith() 함수를 통해 특정 문자가 끝부터 시작하는 행을 검색할 수도 있음\n",
        "\n",
        "\n",
        "df.filter(df.fname.startswith(\"T\")).show()\n",
        "df.filter(df.fname.endswith(\"Cruise\")).show()\n"
      ],
      "metadata": {
        "id": "G9RQzcd1zsQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column\n",
        "# isNull() 함수를 통해 null 값이 있는 행만 출력할 수 있고, isNotNull() 함수를 통해 null 값이 없는 행만 출력할 수 있음\n",
        "\n",
        "df.filter(df.lname.isNull()).show()\n",
        "df.filter(df.lname.isNotNull()).show()"
      ],
      "metadata": {
        "id": "j37uSHUIzzYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column\n",
        "# substr 함수를 통해 특정 컬럼에 원하는 문자열 만큼만 출력할 수 있음\n",
        "\n",
        "df.select(df.fname.substr(1,2).alias(\"substr\")).show()\n"
      ],
      "metadata": {
        "id": "J6KtPvSHz5CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column\n",
        "# when() 함수를 통해 다양한 조건들을 순서적으로 추가하여 만족할때 까지의 구문을 생성 가능\n",
        "# otherwise() 함수도 마지막에 작성하여 그외의 조건으로 명시할 수 있음\n",
        "\n",
        "df.show()\n",
        "\n",
        "from pyspark.sql.functions import when\n",
        "df.select(df.fname,df.lname,when(df.gender==\"M\",\"Male\")\n",
        ".when(df.gender==\"F\",\"Female\")\n",
        ".alias(\"new_gender\")).show()"
      ],
      "metadata": {
        "id": "c0xaE-flz_Mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column\n",
        "# isin() 함수를 통해 특정 값이 존재하는 행만 출력할 수도 있으며, isin() 함수의 입력값은 하나의 정수 값뿐만 아니라 List 형태로 다중 값도 작성할 수 있음\n",
        "\n",
        "li=[\"100\",\"200\"]\n",
        "df.select(df.fname,df.lname,df.id).filter(df.id.isin(li)).show()\n"
      ],
      "metadata": {
        "id": "VeGULvO00dwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select\n",
        "# select() 함수 내에 columns () 함수의 [:] 인덱스 방법을 통해 원하는 컬럼의 길이 만큼 출력 가능\n",
        "\n",
        "df.show()\n",
        "df.select(df.columns[:3]).show(3)\n",
        "df.select(df.columns[2:4]).show(3)"
      ],
      "metadata": {
        "id": "xueTNDau0k7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WithColumn\n",
        "# withColumn() 함수를 통해 컬럼을 생성하거나 변경할 수 있음\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "df2 = df.withColumn(\"id\",col(\"id\").cast(\"Integer\"))\n",
        "df2.printSchema()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "fx1Q8dsQ05oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WidhColumn\n",
        "# col() 함수에 연산을 수행\n",
        "\n",
        "df3 = df.withColumn(\"id\",col(\"id\")*100)\n",
        "df3.show()\n",
        "df.show() "
      ],
      "metadata": {
        "id": "Xg8aVkaI1SPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WithColumnRenamed\n",
        "# WidhColumnRenamed() 함수에 첫 번째 인자는 기존 컬럼명, 두 번째 인자는 변경할 컬럼명을 입력하여 컬럼명을 변경\n",
        "\n",
        "df5 = df.withColumnRenamed(\"id\",\"new_id\")\n",
        "df5.show()\n",
        "df.show()"
      ],
      "metadata": {
        "id": "zXuvhpQE1Zkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WithColumnRenamed\n",
        "# 다수의 컬럼명을 변경하고 싶다면, WithColumnRenamed () 함수에 점(“.”)을 이용하여 계속적으로 함수를 사용\n",
        "\n",
        "df.show()\n",
        "df.withColumnRenamed(\"id\",\"new_id\").withColumnRenamed(\"fname\", \"new_fname\").show()"
      ],
      "metadata": {
        "id": "hBedeIpt1kvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 컬럼명 변경\n",
        "newColumns = [\"newCol1\",\"newCol2\",\"newCol3\",\"newCol4\"]\n",
        "new_df = df.toDF(*newColumns)\n",
        "new_df.show()\n",
        "df.show()"
      ],
      "metadata": {
        "id": "wTT_fMMX10rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop\n",
        "# drop() 함수를 이용하여 컬럼명을 인자값으로 지정하면, Dataframe에서 해당 컬럼을 삭제\n",
        "\n",
        "df6 = df.drop(\"gender\")\n",
        "df6.show()\n",
        "df.show()"
      ],
      "metadata": {
        "id": "mFQenkMV11Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter\n",
        "# filter() 함수를 이용하여 원하는 데이터 출력\n",
        "\n",
        "df.show()\n",
        "df.filter(df.fname == \"James\").show()\n",
        "df.filter(df.fname != \"James\").show() \n",
        "df.filter(~(df.fname == \"James\")).show()"
      ],
      "metadata": {
        "id": "jPm1GG3l11O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()\n",
        "df.filter(\"gender == 'M'\").show()\n",
        "df.filter(\"gender != 'M'\").show()\n",
        "df.filter(\"gender <> 'M'\").show()"
      ],
      "metadata": {
        "id": "BZsBmWUS11XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter\n",
        "# AND/OR/NOT 연산을 지원하기 때문에 다양한 조건 입력이 가능\n",
        "\n",
        "df.show()\n",
        "df.filter((df.gender == \"F\") & (df.fname == \"Ann\")).show(truncate=False)  "
      ],
      "metadata": {
        "id": "s4HOPRJb11eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()\n",
        "df.filter(df.fname.like(\"Tom%\")).show()"
      ],
      "metadata": {
        "id": "O2Pv1RmC249Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 새 df 생성\n",
        "\n",
        "data = [(\"James\", \"Sales\", 3000),\n",
        "  (\"Robert\", \"Sales\", 4100),\n",
        "  (\"Maria\", \"Finance\", 3000),\n",
        "  (\"James\", \"Sales\", 3000),\n",
        "  (\"Scott\", \"Finance\", 3000),\n",
        "  (\"Jen\", \"Finance\", 3900),\n",
        "  (\"Maria\", \"Finance\", 3000),\n",
        "]\n",
        "columns= [\"employee_name\", \"department\", \"salary\"]\n",
        "df = spark.createDataFrame(data = data, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "KPNBl1WH27_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distinct\n",
        "# Dataframe에서 중복을 제거하기 위한 함수\n",
        "\n",
        "distinctDF = df.distinct()\n",
        "print(\"Distinct count: \"+str(distinctDF.count()))\n",
        "distinctDF.show(truncate=False)\n",
        "\n",
        "df2 = df.dropDuplicates()\n",
        "print(\"Distinct count: \"+str(df2.count()))\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "id": "rb9GMNi93HYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 다수 컬럼 지정\n",
        "\n",
        "df.show()\n",
        "dropDisDF = df.dropDuplicates([\"department\",\"salary\"])\n",
        "print(\"Distinct count of department & salary : \"+str(dropDisDF.count()))\n",
        "dropDisDF.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "V7nlhFfK3OkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort\n",
        "# 컬럼의 데이터를 정렬\n",
        "\n",
        "df.show()\n",
        "df.sort(\"department\",\"salary\").show()\n",
        "df.sort(col(\"department\"),col(\"salary\")).show()\n"
      ],
      "metadata": {
        "id": "Xp70jgKa37Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort\n",
        "# orderBy() 함수를 사용하여 동일한 기능을 수행\n",
        "\n",
        "df.orderBy(\"department\",\"salary\").show()\n",
        "df.orderBy(col(\"department\"),col(\"salary\")).show()\n"
      ],
      "metadata": {
        "id": "fTdK_fsM4XGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort\n",
        "# 오름차순, 내림차순은 지정한 컬럼명 뒤에 .asc()와 .desc()를 사용하여 정렬을 수행\n",
        "\n",
        "df.sort(df.department.asc(),df.salary.desc()).show()\n",
        "df.sort(col(\"department\").asc(),col(\"salary\").desc()).show()\n",
        "df.orderBy(col(\"department\").asc(),col(\"salary\").desc()).show()\n"
      ],
      "metadata": {
        "id": "Jdagbg5e4tMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL처럼 사용하고 싶다면 ViewFrame 생성해서 가능\n",
        "\n",
        "df.createOrReplaceTempView(\"ViewFrame\")\n",
        "spark.sql(\"select department, salary from ViewFrame ORDER BY department asc\").show()"
      ],
      "metadata": {
        "id": "9XbkgPCG43SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# groupBy\n",
        "# groupBy() 함수를 통해 원하는 컬럼을 그룹화하여 다양한 집계를 수행할 수 있음\n",
        "\n",
        "df.show()\n",
        "df.groupBy(\"department\").count().show()"
      ],
      "metadata": {
        "id": "yaeq-ctl4-xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# groupBy\n",
        "# \"department” 컬럼을 기준으로 그룹화하고, sum(), min(), max(), avg(), mean() 연산(집계) 수행\n",
        "\n",
        "df.groupBy(\"department\").sum(\"salary\").show()\n",
        "df.groupBy(\"department\").min(\"salary\").show()\n",
        "df.groupBy(\"department\").max(\"salary\").show()\n",
        "df.groupBy(\"department\").avg( \"salary\").show()\n",
        "df.groupBy(\"department\").mean( \"salary\") .show()"
      ],
      "metadata": {
        "id": "pFvtL6bn5FAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  groupBy\n",
        "# groupBy() 함수 내의 다수의 컬럼명을 지정하여 다수의 컬럼명을 그룹화가 가능\n",
        "\n",
        "df.groupBy(\"employee_name\",\"department\").sum(\"salary\").show()\n"
      ],
      "metadata": {
        "id": "sUX9CnPX5NjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join\n",
        "# Join을 위해 2개의 DF생성\n",
        "\n",
        "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000),\n",
        "(2,\"Rose\",1,\"2010\",\"20\",\"M\",4000),\n",
        "(3,\"Williams\",1,\"2010\",\"10\",\"M\",1000),\n",
        "(4,\"Jones\",2,\"2005\",\"10\",\"F\",2000),\n",
        "(5,\"Brown\",2,\"2010\",\"40\",\"\",-1),\n",
        "(6,\"Brown\",2,\"2010\",\"50\",\"\",-1)]\n",
        "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\",\"emp_dept_id\",\"gender\",\"salary\"]\n",
        "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
        "empDF.printSchema()\n",
        "empDF.show(truncate=False)\n",
        "\n",
        "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
        "deptDF.printSchema()\n",
        "deptDF.show(truncate=False)"
      ],
      "metadata": {
        "id": "zzpl5RWK5U9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join\n",
        "# Inner Join\n",
        "\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\").show()"
      ],
      "metadata": {
        "id": "I9eL1frB5gao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join\n",
        "# full outer join\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\").show()\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\").show()\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\").show()"
      ],
      "metadata": {
        "id": "02Q6SrpA5l2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join\n",
        "# left outer join\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\").show()\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\").show()"
      ],
      "metadata": {
        "id": "7AbdY1ai5o8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join\n",
        "#right outer join\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\").show()\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"rightouter\").show()"
      ],
      "metadata": {
        "id": "T6foiReX5r3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join\n",
        "#left semi join\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\").show()"
      ],
      "metadata": {
        "id": "_HRbix4V5wbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join\n",
        "#left anti join\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\").show()"
      ],
      "metadata": {
        "id": "aPQjvCVq5zdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join\n",
        "#self join\n",
        "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), \n",
        "col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\").select(col(\"emp1.emp_id\"),col(\"emp1.name\"),\n",
        "col(\"emp2.emp_id\").alias(\"superior_emp_id\"), col(\"emp2.name\").alias(\"superior_emp_name\")).show()\n"
      ],
      "metadata": {
        "id": "hSbc-k1O53Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Union\n",
        "# 병합을 위해 2개의 DF생성\n",
        "\n",
        "simpleData = [(\"A\",\"B\",\"C\"), (\"A\",\"B\",\"D\"), (\"A\",\"B\",\"E\"), (\"A\",\"B\",\"F\"), (\"A\",\"B\",\"D\")]\n",
        "columns = [\"col_1\",\"col_2\",\"col_3\"]\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "simpleData2 = [(\"A\",\"B\",\"G\"), (\"A\",\"B\",\"H\"), (\"A\",\"B\",\"I\"), (\"A\",\"B\",\"F\"), (\"A\",\"B\",\"D\")]\n",
        "columns2 = [\"col_1\",\"col_2\",\"col_3\"]\n",
        "df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "id": "Z1OU_JRU6CED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Union\n",
        "# Dataframe 변수를 명시하고, .union(“합병할 Dataframe명”)을 입력\n",
        "\n",
        "unionDF = df.union(df2)\n",
        "unionDF.show()\n",
        "\n",
        "disDF = df.union(df2).distinct()\n",
        "disDF.show()"
      ],
      "metadata": {
        "id": "1t9QdQu76HJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Union\n",
        "# UDF(User Defined Function)\n",
        "# 일반 Python 함수를 생성하고, PySpark 관련 명령어에서 불러서 편리하게 사용이 가능함\n",
        "# converCast() 함수를 생성\n",
        "\n",
        "def convertCase(str):\n",
        "    resStr=\"\"\n",
        "    arr = str.split(\" \")\n",
        "    for x in arr:\n",
        "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
        "    return resStr\n",
        "\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),(\"2\", \"tracey smith\"),(\"3\", \"amy sanders\")]\n",
        "df = spark.createDataFrame(data,columns)\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "EkN4IKfs6O7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (User Defined Function)\n",
        "# 작성한 함수를 바로는 사용할 수 없고, PySpark의 udf 함수를 우선 적용하여야함\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "convertUDF = udf(lambda z: convertCase(z))\n",
        "df.select(col(\"Seqno\"),convertUDF(col(\"Name\")).alias(\"Name\") ).show()\n"
      ],
      "metadata": {
        "id": "wOqVZ4Ni6_a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map vs flatmap\n",
        "# map()은 RDD 각 요소에 함수를 적용\n",
        "# flatmap()은 map으로 반환된 결과의 각 요소를 반환\n",
        "\n",
        "rdd = sc.parallelize(['A,B,C','D,E,F','G,H,I'])\n",
        "\n",
        "map_rdd = rdd.map(lambda x: x.split(','))\n",
        "print(map_rdd.collect())"
      ],
      "metadata": {
        "id": "_gQpUMJj7JwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map vs flatmap\n",
        "# Dataframe에 적용하면, map()을 통해 구성된 RDD 데이터를 출력하면, 하나의 행이 하나의 튜플로 구성되어 출력\n",
        "# flatmap()을 통해 구성된 RDD 데이터를 출력하면, 하나의 행이 하나의 튜플이 아닌 모든 행이 한번에 처리되어 출력\n",
        "\n",
        "data = [('James','Smith','M',30),('Anna','Rose','F',41),('Robert','Williams','M',62)]\n",
        "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "df.show()\n",
        "\n",
        "rdd2=df.rdd.map(lambda x: (x[0] + \",\" + x[1], x[2], x[3]*2))\n",
        "print(rdd2.collect())\n",
        "\n",
        "rdd3=df.rdd.flatMap(lambda x: (x[0] + \",\" + x[1], x[2], x[3]*2))  \n",
        "print(rdd3.collect())"
      ],
      "metadata": {
        "id": "cuwgKB3h7TH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample\n",
        "# sample() 함수는 데이터에서 무작위로 추출할 수 있는 기능\n",
        "\n",
        "df=spark.range(20)\n",
        "sample_df = df.sample(0.2)\n",
        "sample_df.show()"
      ],
      "metadata": {
        "id": "2B4rFpAM7uhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시드값 추가\n",
        "\n",
        "df.sample(0.2,123).show()\n",
        "df.sample(0.2,123).show()\n",
        "df.sample(0.2,456).show()\n",
        "df.sample(0.2,456).show()\n",
        "\n",
        "# True를 추가하면 중복 허용\n",
        "df.sample(True,0.2).show()\n",
        "df.sample(0.2).show()"
      ],
      "metadata": {
        "id": "oEZEoXvZ75q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "f = files.upload()"
      ],
      "metadata": {
        "id": "vamyW3sZ8VXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fTIC6UNa8WKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fill\n",
        "# fill() 함수는 Dataframe 데이터가 0, 빈 문자열, 공백, NULL 등일때 전처리를 할 수 있는 기능\n",
        "\n",
        "df = spark.read.options(header='true', inferSchema='true').csv(\"animal_data_img.csv\")\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "wY-4NStL78WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fill\n",
        "# fill() 함수는 Dataframe 데이터가 0, 빈 문자열, 공백, NULL 등일때 전처리를 할 수 있는 기능\n",
        "\n",
        "df.na.fill(value=0).show()\n",
        "df.na.fill(value=0,subset=[\"Animal_Type\"]).show()\n"
      ],
      "metadata": {
        "id": "3YvrAVz28e7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5_idfRQjVxLh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}